\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage[margin=0.75in]{geometry}

%opening
\title{CSC2621: Assignment 1\\Due Tuesday, February 5  at 6pm}
\author{}

\def\code#1{\texttt{#1}}

\begin{document}

\maketitle


\section{Introduction}
The goal of this assignment is to help you get familiar with imitation learning and DAgger applied to the driving domain, and to also ensure that you are able to solve supervised learning
tasks with neural networks. In order to automate the process of expert demonstrations, without relying on human input, we have modified a car racing environment in OpenAI Gym to also provide 
expert demonstrations from a feedback controller that allows the car to do road following. Unlike the feedback controller that has access to the true state and coordinates of the track, your 
driving policy will only have access to an image and will output a steering command. 


\section{Setting Up}
This assignment assumes that you are running some version of Ubuntu between 16.04 to 18.04. If you are a Windows or Mac user and you are experiencing issues, please
email the instructor early on to try to resolve them.
\newline

\noindent \textbf{Starter code} Run \code{git clone https://github.com/florianshkurti/csc2621w19.git} in order to get the starter code for this assignment. This should create the directory
\code{csc2621w19}.
\newline

\noindent \textbf{Virtualenv} Set up a python virtualenv environment in the directory \code{csc2621w19/assignments/}, so that you can install python packages without requiring administrative
rights in the computer you are using. Specifically, run \code{virtualenv -p python3 myenv}, which will setup a Python3 environment under the directory \code{myenv}. Run \code{source ./myenv/bin/activate}
in order to activate that virtual environment. Now you can begin installing python dependencies. Start with \code{pip3 install scikit-image}
\newline

\noindent \textbf{Pytorch} Follow the instructions at \url{https://pytorch.org/get-started/locally/} to install Pytorch 1.0 for CPU or GPU, depending on whether your system has access to a GPU.
If you have a GPU you will also need to know which version of CUDA is your system running. You can do this by executing \code{nvcc --version}. If you don't have a GPU select \code{None} when choosing
the CUDA version.
\newline

\noindent \textbf{OpenAI Gym} Run \code{pip3 install gym} and then \code{pip3 install box2d-py}. This assignment has been tested with Gym version \code{0.10.9}. 
\newline

\noindent \textbf{Initial training set} The assignment repository includes an initial dataset of images annotated with expert actions, which you can use for the first round of training a steering network.
Run \code{cd assignments/A1} and then \code{unzip dataset.zip}.
\newline

\noindent \textbf{Run the expert} Try to run \code{python3 racer.py --expert\_drives=True}. If you see a car driving in the middle of the track your installation works. The expert in this case is a feedback
controller with only 4 parameters that have been tuned. However, the expert has access to extra information, such as the distance of the car from the middle of the road, while your learner will only have
access to the image. 


% \begin{figure}[h]
%   \begin{center}
%     \includegraphics[width=0.5\textwidth]{gmapping}
%   \end{center} 
%   \caption{Part of the robot's mapping task during the (0,0) noise trajectory.}
%   \label{fig:gmapping}
% \end{figure}
%\noindent What you need to submit: 5 images of maps produced by your mapper, with noise parameters (0, 0), (2.5, 0), (5, 0), (2.5, 0.1), (5, 0.1) respectively. 
%Your images should be named \path{map_[0|1|2|3|4]_firstname_lastname.png}. Also, submit a video recording of the rviz visualization for the odometry noise combination (0,0),
%demonstrating your map being built from beginning to end. Your video should be named \path{map_0_firstname_lastname.mp4/avi/ogg}. 

\section{Supervised Learning (10 pts)}
Fill in the code for the training procedure in \code{train\_policy.py}. If you have a GPU make sure that in \code{utils.py} the device is set to \code{cuda}. Otherwise, set it to \code{cpu}.   
To start the training procedure run \code{python3 train\_policy.py --n\_epochs=50 --batch\_size=256 --weights\_out\_file=./weights/learner\_0\_supervised\_learning.weights}
\newline \code{--train\_dir=./dataset/train/ --weighted\_loss=False}
\newline

\noindent Also fill the network architecture in \code{driving\_policy.py}. Start with an architecture that is similar, but not identical, to the NVIDIA end-to-end driving paper, even though it likely has too many parameters 
for fitting the dataset used in this assignment. For the features of the convolutional network start with this sequential model: \code{24 conv, relu, 36 conv, relu, 48 conv, relu, 64 conv, relu, flatten}, where each 
convolutional layer (called Conv2d in Pytorch) uses a kernel/window of size 4, stride of size 2, and 1 pixel padding. For the classifier start with this fully connected model: \code{fc 4096, relu, fc 128, relu, fc 64, 
relu, fc n\_classes, relu} where \code{fc} stands for a fully-connected layer (called Linear layer in Pytorch). The output of the network will be the probability of \code{n\_class} possible steering directions,
just like in the ALVINN paper. You are not however required to implement the gaussian-shaped discrete predictions that Pomerlau used to make consecutive classes nearby in terms of the angle they represent. 
You should optimize the categorical cross-entropy loss to learn a mapping from images to actions.   
\newline           

\noindent Now, try to execute the learner's policy on the simulator. You should see that the initial policy eventually gets outside the race track before completing one full round of driving. You
should also observe that the steering policy is wobbly on straight lines. You are not required to fix that for the purposes of this assignment. 


\section{Weighted Classification Loss (5 pts)}
The majority of the expert demonstrations you will have in your dataset will be for driving straight ahead. Your training set will have a significant class imbalance, where the labels corresponding to
driving straight ahead (e.g. classes {9,10,11} if \code{n\_classes=20}) will occur much more frequently than sharp turns. One reasonable question to ask is whether taking this class imbalance into account
when training the learner could help. To answer that question use a weighted loss that weighs errors in each class according to the inverse frequency of occurrence. In Pytorch the \code{cross\_entropy} loss 
function allows you to specify such weights. Implement this minor change in \code{train\_policy.py} and rerun supervised learning with the weighted loss: \code{python3  train\_policy.py --n\_epochs=50 
--batch\_size=256 --weights\_out\_file=./weights/learner\_0\_weighted\_loss.weights} \newline \code{--train\_dir=./dataset/train/ --weighted\_loss=True}
\newline

\noindent You will see that the vehicle still veers off track. 

\section{DAgger (5 pts)}
Implement DAgger in \code{dagger.py} and run \code{python3 dagger.py}. For 10 DAgger iterations you will need to wait about 6-7 hours if you have a GPU or about double that if you only have a CPU. If you
face any problems with this please let me know. This process should generate weights \code{learner\_0.weights, ..., learner\_10.weights} at the end of each DAgger iteration. Also, keep in mind that when you 
run this procedure the training set in \code{dataset/train} will be augmented with expert-labeled images from the execution of the learner's policy. Images from the $i^{\text{th}}$ DAgger run will be saved under 
\code{dataset/train/expert\_i\_t\_cmd.jpg}, where \code{t} is the timestep of the image recorded during the $i^{\text{th}}$ run, and \code{cmd} is the corresponding steering command that the expert issued for 
that image in [-1,1].  

\noindent You should observe that as the training set increases the resulting policy will eventually be able to remain within the track and fully traverse it.

\section{What/How to submit}
Submit a file called \path{assignment_firstname_lastname_studentid.zip} that contains the starter code and your extensions to the provided starter code, under the directory \code{assignments/A1}. 
This zip file will include your changes to the files \code{train\_policy.py, driving\_policy.py, dagger.py} and the learner weights for each question (12 weight files in total). Submissions should be 
done on Quercus. You do not need to include a writeup of your solutions. 

\end{document}